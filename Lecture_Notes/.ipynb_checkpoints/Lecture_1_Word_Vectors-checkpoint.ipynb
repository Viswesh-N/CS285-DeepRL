{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d47eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2021-07-31\n",
    "# GitHub: https://github.com/jaaack-wang \n",
    "# About: Intro and word vectors for Stanford CS224N- NLP with Deep Learning | Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07289ba3",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [1. Casual takeaways](#1)\n",
    "- [2. WordNet](#2)\n",
    "    - [2.1 Quick facts](#2-1)\n",
    "    - [2.2 Accessing WordNet](#2-2)\n",
    "    - [2.3 Limitations](#2-3)\n",
    "- [3. Word vectors](#3)\n",
    "    - [3.1 One-hot vectors](#3-1)\n",
    "    - [3.2 Word vectors (embeddings)](#3-2)\n",
    "    - [3.3 Word2vec algorithm basics](#3-3)\n",
    "        * [3.3.1 Basic ideas](#3-3-1)\n",
    "        * [3.3.2 Math behind](#3-3-2)\n",
    "- [4. References](#4)\n",
    "- [5. Appendix: gradients for loss function (2)](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da267f",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1. Casual takeaways\n",
    "\n",
    "- Language isn't a formal system. Language is glorious chaos. Quoted from https://xkcd.com/1576/\n",
    "- Commonest linguistic way of thinking of meanings: signifier (symbol) $\\Longleftrightarrow$ signified (idea or thing) \n",
    "    - = denotational semantics. \n",
    "    - <font color='blue'>Note: <font color='black'>This idea has been poplularized by Ferdinand de Saussure in the his <i>Course in General Linguistics</i> where he, however, proposed that <ins>\"language is a formal system of signs that express idea\"</ins>. [More](https://en.wikipedia.org/wiki/Course_in_General_Linguistics).\n",
    "- Ways of word meanings represented by computers:\n",
    "    - [WordNet](#2): A traditional and manually labelled way.\n",
    "    - [Words vectors](#3):  \n",
    "        - [One-hot vectors](#3-1)\n",
    "        - [Word vectors (embeddings)](3-2)\n",
    "- NLTK is sort like \"Swiss Army Knife of NLP\", meaning that it's not terribly good for anything, but has a lot of basic tools. LOL.\n",
    "- In traditional NLP, we regard words as discrete symbols: one-hot encoding until 2012. \n",
    "- “You shall know a word by the company it keeps” (J. R. Firth 1957: 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf5a86",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2. WordNet\n",
    "\n",
    "Unless specifically stated, the term WordNet refers to the one designed for English language by Princeton \n",
    "\n",
    "<a name='2-1'></a>\n",
    "## 2.1 Quick facts\n",
    "\n",
    "- WordNet was first created in English only in the Cognitive Science Laboratory (Department of Psychology) of Princeton University starting in 1985, but it is currently housed in the Department of Computer Science.\n",
    "- The most recent Windows version of WordNet is 2.1, released in March 2005. Version 3.0 for Unix/Linux/Solaris/etc. was released in December 2006. Version 3.1 is currently available only online.\n",
    "- WordNet® is a large lexical database of semantic relations between words in English. It **includes the lexical categories nouns, verbs, adjectives and adverbs but ignores prepositions, determiners and other function words.**\n",
    "- The main relation among words in WordNet is synonymy. **Synonyms--words that denote the same concept and are interchangeable in many contexts--are grouped into unordered sets (synsets).** Other semantic relations include: hyponyms, and meronyms etc.\n",
    "- WordNet for other languages: https://en.wikipedia.org/wiki/WordNet#Other_languages \n",
    "\n",
    "<a name='2-2'></a>\n",
    "## 2.2 Accessing WordNet\n",
    "- Web interface: http://wordnetweb.princeton.edu/perl/webwn\n",
    "- Official Downloads for associated packages and tools: https://wordnet.princeton.edu/download\n",
    "- Python using [nltk](https://www.nltk.org)(Natural Language Toolkit). Below is example code: \n",
    "    - First, make sure that you have nltk installed. Run the command:  \n",
    "```\n",
    "pip install --user -U nltk\n",
    "```\n",
    "    - Second, make sure you have the WordNet dataset installed. In python/ipython, run:\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "```\n",
    "    - Then, you can try the following code to utilize WordNet (A more detailed guide: http://www.nltk.org/howto/wordnet.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea64032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets for the word \"invite\" in WordNet:\n",
      "\n",
      " [Synset('invite.n.01'), Synset('invite.v.01'), Synset('invite.v.02'), Synset('tempt.v.03'), Synset('invite.v.04'), Synset('invite.v.05'), Synset('invite.v.06'), Synset('invite.v.07'), Synset('receive.v.05')]\n"
     ]
    }
   ],
   "source": [
    "# commmon way to import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "# look up a word using synsets(): a set of synonyms that share a common meaning.\n",
    "# a synset is identified with a 3-part name of the form: word.pos.nn\n",
    "print('Synsets for the word \"invite\" in WordNet:\\n\\n', wn.synsets('invite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af598d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Way one--------------------\n",
      "Synsets for the noun \"invite\" in WordNet:\n",
      "\n",
      " [Synset('invite.n.01')]\n",
      "\n",
      "\n",
      "--------------------Way two--------------------\n",
      "Synsets for the noun \"invite\" in WordNet:\n",
      "\n",
      " [Synset('invite.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# We can constrain the search by specifying the part of speech\n",
    "# parts of speech available: ADJ, ADV, ADJ_SAT, NOUN, VERB\n",
    "# ADJ_SAT: see https://stackoverflow.com/questions/18817396/what-part-of-speech-does-s-stand-for-in-wordnet-synsets\n",
    "\n",
    "# Way one\n",
    "print(f'{\"-\"*20}Way one{\"-\"*20}')\n",
    "print('Synsets for the noun \"invite\" in WordNet:\\n\\n', wn.synsets('invite', pos=wn.NOUN))\n",
    "\n",
    "# Way two\n",
    "print(f'\\n\\n{\"-\"*20}Way two{\"-\"*20}')\n",
    "# pos: {'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "print('Synsets for the noun \"invite\" in WordNet:\\n\\n', [s for s in wn.synsets('invite') if s.pos()=='n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a191ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Definition--------------------\n",
      "The definition for invite as a noun:\n",
      "\n",
      " a colloquial expression for invitation\n",
      "\n",
      "\n",
      "--------------------Examples--------------------\n",
      "The definition for invite as a noun:\n",
      "\n",
      " [\"he didn't get no invite to the party\"]\n",
      "\n",
      "\n",
      "--------------------Hypernyms--------------------\n",
      "The hypernyms for invite as a noun:\n",
      "\n",
      " [Synset('invitation.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# check definition of a synset\n",
    "print(f'{\"-\"*20}Definition{\"-\"*20}')\n",
    "print('The definition for invite as a noun:\\n\\n', wn.synset('invite.n.01').definition())\n",
    "\n",
    "# check the related examples\n",
    "print(f'\\n\\n{\"-\"*20}Examples{\"-\"*20}')\n",
    "print('The definition for invite as a noun:\\n\\n', wn.synset('invite.n.01').examples())\n",
    "\n",
    "# check the hypernyms\n",
    "print(f'\\n\\n{\"-\"*20}Hypernyms{\"-\"*20}')\n",
    "print('The hypernyms for invite as a noun:\\n\\n', wn.synset('invite.n.01').hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2804a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('entity.n.01')\n",
      "Synset('physical_entity.n.01')\n",
      "Synset('abstraction.n.06')\n",
      "Synset('thing.n.12')\n",
      "Synset('object.n.01')\n",
      "Synset('whole.n.02')\n"
     ]
    }
   ],
   "source": [
    "# check all Synsets availble by wn.all_synsets()\n",
    "# we can also specify the part of speech here\n",
    "\n",
    "for idx, noun in enumerate(wn.all_synsets('n')):\n",
    "    print(noun)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c9714",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "## 2.3 Limitations\n",
    "- Great as a resource but missing nuance\n",
    "• e.g., “proficient” is listed as a synonym for “good”\n",
    "This is only correct in some contexts\n",
    "- Missing new meanings of words\n",
    "• e.g., wicked, badass, nifty, wizard, genius, ninja, bombest\n",
    "- Impossible to keep up-to-date!\n",
    "- Subjective\n",
    "- **Requires human labor to create and adapt**\n",
    "- Can’t compute accurate word similarity\n",
    "    - nltk does provide a way to compute word similarity, but it is too simplistic and subjective: synset1.path_similarity(synset2): Return a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1. See: http://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cbb454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path similarity between cat(noun) and dog(noun):  0.2\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print('The path similarity between cat(noun) and dog(noun): ', dog.path_similarity(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5980a5",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3. Word vectors\n",
    "\n",
    "My notes regarding word vectors and word2vec algorithms: https://github.com/jaaack-wang/dl-nlp-using-paddlenlp/tree/main/paddlenlp_updated_notes_English/WordEmbedding.\n",
    "\n",
    "<a name='3-1'></a>\n",
    "## 3.1 One-hot vectors\n",
    "- Representing words as discrete symbols. \n",
    "- Popular in traditional NLP till 2012 where neural network trained word embeddings took place\n",
    "\n",
    "**Problems**:\n",
    "- The vocabulary size can be very large.\n",
    "- Does not show any association between words as any two one-hot vectors are orthogonal. e.g., in web search, \"seattle motel\" != \"seattle hotel\".\n",
    "\n",
    "**Solutions**:\n",
    "- Could try to rely on WordNet’s list of synonyms to get similarity? • But it is well-known to fail badly: incompleteness, etc.\n",
    "- Instead: learn to encode similarity in the vectors themselves.\n",
    "\n",
    "<a name='3-2'></a>\n",
    "## 3.2 Word vectors (embeddings)\n",
    "\n",
    "**Basic ideas**:\n",
    "- Distributional semantics: A word’s meaning is given by the words that frequently appear close-by.\n",
    "- When a word w appears in a text, its context is the set of words that appear nearby (within a fixed-size window).\n",
    "- Use the many contexts of w to build up a representation of w\n",
    "- We will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts\n",
    "\n",
    "\n",
    "<a name='3-3'></a>\n",
    "## 3.3 Word2vec algorithm basics\n",
    "\n",
    "Word2vec (Mikolov et al. 2013) is a framework for learning word vectors\n",
    "\n",
    "<a name='3-3-1'></a>\n",
    "### 3.3.1 Basic ideas \n",
    "- We have a large corpus (“body”) of text\n",
    "- Every word in a fixed vocabulary is represented by a vector\n",
    "- Go through each position t in the text, which has **a center word $c$** and **context (“outside”) words $o$**\n",
    "- Use the similarity of the word vectors for $c$ and $o$ to calculate the probability of $o$ given $c$ (or vice versa)\n",
    "- Keep adjusting the word vectors to maximize this probability\n",
    "\n",
    "<img src='../images/1-word2vec-example.png' width='800' height='300'>\n",
    "\n",
    "<a name='3-3-2'></a>\n",
    "### 3.3.2 Math\n",
    "\n",
    "**Objective/loss/cost function**\n",
    "\n",
    "For each position $t = 1, ... , 𝑇$, predict context words within a window of fixed size $m$, given center word $w_j$. Data likelihood:\n",
    "\n",
    "$$Likelihood = L(\\theta) = \\prod_{t}^{T} \\prod_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} P(w_{t+j}|{w_{t}; \\theta}) \\tag{1}$$\n",
    "\n",
    "The objective/loss/cost function for $(1)$ is the average negative log likelihood: \n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{T} logL(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} log P(w_{t+j}|{w_{t}; \\theta}) \\tag{2}$$\n",
    "\n",
    "\n",
    "**Prediction function**\n",
    "\n",
    "Denote by $v_{c}$ and $v_{o}$ respectively the center word and the context word, using **softmax**, we get the following prediction function of predicting $v_{c}$ given $v_{o}$ and the vocubulary $V$:\n",
    "\n",
    "$$P(o|c) = \\frac{exp(\\mathbf{u_{o}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\tag{3}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='../images/1-softmax.png' width='800' height='300'>\n",
    "\n",
    "**Gradient descent**\n",
    "\n",
    "Knowledge needed: calculus: especially the chain rule, multivaraite calculus, partial derivative.   \n",
    "\n",
    "In this lecture slide, Christopher Manning only introduced Batch Gradient Descent and Stochastic Gradient Descent, but not Mini-batch Gradient Descent (he did in the second lecture). Using Mini-batch Gradient Descent, we will update the parameters after looping through k examples (usually 16, 32, 64, $2^n$...). In a way, Batch Gradient Descent (update the parameters after looping through all the examples) and Stochastic Gradient Descent (update the parameters for every example encountered) are two extreme instances of Mini-batch Gradient Descent. \n",
    "\n",
    "\n",
    "<font color='red'><b>Check the details of how to derive the gradients for the loss function $(2)$ in the Appendix.</b></font>\n",
    "\n",
    "<img src='../images/1-gradient-descent-1.png' width='800' height='300'>\n",
    "\n",
    "<img src='../images/1-gradient-descent-2.png' width='800' height='300'>\n",
    "\n",
    "<img src='../images/1-gradient-descent-3.png' width='800' height='300'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62cbe5",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# 4. References\n",
    "\n",
    "\n",
    "- [Course website](http://web.stanford.edu/class/cs224n/index.html)\n",
    "\n",
    "- [Lecture video](https://www.youtube.com/watch?v=8rXD5-xhemo&t=732s) \n",
    "\n",
    "- [Lecture slide](http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture01-wordvecs1.pdf)\n",
    "\n",
    "- [WordNet official website](https://wordnet.princeton.edu)\n",
    "\n",
    "- [Wikipedia WordNet entry](https://en.wikipedia.org/wiki/WordNet#Other_languages)\n",
    "\n",
    "- [NLTL WordNet Interface](http://www.nltk.org/howto/wordnet.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fdb11",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "# 5. Appendix: gradients for loss function (2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb92beb",
   "metadata": {},
   "source": [
    "<font color='red'>Alternative using a different but neater set of notations: [word2vec gradients](https://courses.cs.ut.ee/MTAT.03.277/2015_fall/uploads/Main/word2vec.pdf)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9f58e",
   "metadata": {},
   "source": [
    "### <font color='blue'>Formula review</font>\n",
    "\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{T} logL(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} log P(w_{t+j}|{w_{t}; \\theta}) \\tag{2}$$\n",
    "\n",
    "where $\\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq m}} log P(w_{t+j}|{w_{t}; \\theta})$ is equal to $log$ formula (3):\n",
    "\n",
    "$$log P(o|c) = log \\frac{exp(\\mathbf{u_{o}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa4e11",
   "metadata": {},
   "source": [
    "### <font color='blue'>Some basic rules we need</font>\n",
    "    \n",
    "**Basic rules for logarithms**\n",
    "\n",
    "- $log (\\frac{a}{b}) = log(a) - log(b)$\n",
    "- $log(e) = 1$ ($log$ here means $ln$)\n",
    "- $log(a^b) = b \\cdot log(a)$\n",
    "- $log(e^x) = x$ ($log$ here means $ln$)\n",
    "\n",
    "\n",
    "**Basic rules for derivatives**\n",
    "\n",
    "- Addition and subtraction: $\\frac{d}{dx} a(x) \\pm b(x) = \\frac{d}{dx} a(x) + \\frac{d}{dx} b(x)$, e.g., $(x + x^2)' = (x)' + (x^2)' = 1 + 2x$\n",
    "\n",
    "\n",
    "- Thus, constant and summation can be taken out front: $\\frac{d}{dx} k \\cdot a(x) = k \\cdot a'(x)$, $\\frac{d}{dx} \\sum_{i=i}^{n} x_i = \\sum_{i=i}^{n} \\frac{d}{dx} x_i = \\sum_{i=i}^{n} x_i' $\n",
    "\n",
    "\n",
    "- Derivatives involving vectors: remember that $\\mathbf{a^{T}b} = \\sum_{i=1}^{n} a_i b_i$, suppose $a_i$ is a set of constants, thus $\\frac{d}{db} \\mathbf{a^{T}b} = \\frac{d}{db} \\sum_{i=1}^{n} a_i b_i = \\sum_{i=1}^{n} a_i = \\mathbf{a}$. In short, $\\frac{d}{db} \\mathbf{a^{T}b} = \\mathbf{a}$. Similarly, this also holds: $\\frac{d}{db} \\mathbf{b^{T}a} = \\mathbf{a}$\n",
    "\n",
    "\n",
    "- Derivatives of exponential terms: $\\frac{d}{dx} x^{k} = k \\cdot x^{k-1}$, e.g., $(x^5)' = 5x^4$. Special case: $\\frac{d}{dx} e^x = e^x$\n",
    "\n",
    "\n",
    "- Product rule: $\\frac{d}{dx} a(x) \\cdot b(x) = b(x) \\cdot \\frac{d}{dx} a(x) + a(x) \\cdot \\frac{d}{dx} b(x) = b(x) \\cdot a(x)' + a(x) \\cdot b(x)'$, e.g., $(x \\times x^2)' = x^2 \\times 1 + x \\times 2x = 3x^2 = (x^3)'$\n",
    "\n",
    "\n",
    "- Chain rule: $\\frac{d}{dx} a(b(x)) = \\frac{d}{db} a(b(x)) \\cdot \\frac{d}{dx} b(x) = a'(b(x)) \\cdot b'(x)$, e.g., suppose $a = n ^ 2$, $b = x + 1$, thus $a(b(x)) = (x+1)^2$. $\\frac{d}{dx} a(b(x)) = a'(b(x)) \\cdot b'(x) = 2(x+1) \\times 1 = 2x+2 = (x^2 +2x + 1)'$\n",
    "\n",
    "\n",
    "- Derivative of log: $\\frac{d}{dx} log(x) = \\frac{1}{log(x)}$\n",
    "\n",
    "\n",
    "**Partial derivatives**\n",
    "\n",
    "The idea of partial derivatives is to take the derivatives with regard to a variable and take the other variables, if any, as constant. For example, if we have functions $a$ and $b$, then the partial derivative with regard to function $a$ in the expresssion $ab$ is $b$, the first part of what is supposed to be under product rule. We use $\\frac{\\partial}{\\partial a}$ to denote the partial derivative with regard to $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e9e26",
   "metadata": {},
   "source": [
    "### <font color='blue'> Let's crack the gradients for the loss function (2)!\n",
    "    \n",
    "Let's convert the loss function (2) using formula (4) and some logrithmic rules introduced above:\n",
    "    \n",
    "$$- \\frac{1}{T} \\log \\frac{exp(\\mathbf{u_{o}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} = - \\frac{1}{T}(\\log exp(\\mathbf{u_{o}^{T} v_{c}}) - \\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})) = - \\frac{1}{T}(\\mathbf{u_{o}^{T} v_{c}}- \\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}))$$\n",
    "\n",
    "\n",
    "In short, we get:\n",
    "    \n",
    "\n",
    "$$J(\\mathbf{v_{c} \\;|\\; u_{o}}) = - \\frac{1}{T}(\\mathbf{u_{o}^{T} v_{c}}- \\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})) \\tag{5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b86f6bd",
   "metadata": {},
   "source": [
    "### Partial derivative with regard to $v_c$\n",
    "    \n",
    "Let's derive the partial derivative with regard to $v_c$ (center word vector) first and then move on to $u_{o}$ (context word vector).\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v_c}} (- \\frac{1}{T}(\\mathbf{u_{o}^{T} v_{c}}- \\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}))) = - \\frac{1}{T} (\\frac{\\partial}{\\partial \\mathbf{v_c}}\\mathbf{u_{o}^{T} v_{c}} -  \\frac{\\partial}{\\partial \\mathbf{v_c}}\\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v_c}}\\mathbf{u_{o}^{T} v_{c}} = \\mathbf{u_o} \\tag{Step 1}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v_c}}\\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}) = \\frac{1}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\cdot \\sum_{x=1}^{V} \\frac{\\partial}{\\partial \\mathbf{v_c}}exp(\\mathbf{u_{x}^{T} v_{c}})\\tag{Step 2}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<font color='grey'>(Please note that, here we did a small trick by converting $\\sum_{w \\in V}$ into $\\sum_{x=1}^{V}$ so that the partial derivative can be more easily computed.)</font>\n",
    "<br>\n",
    "Where\n",
    "\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v_c}}exp(\\mathbf{u_{x}^{T} v_{c}}) = exp(\\mathbf{u_{x}^{T} v_{c}}) \\cdot \\frac{\\partial}{\\partial \\mathbf{v_c}}\\mathbf{u_{x}^{T} v_{c}} = exp(\\mathbf{u_{x}^{T} v_{c}}) \\cdot u_{x}\\tag{Step 3}$$\n",
    "\n",
    "\n",
    "Taking Step 2 and Step3 together, we get:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v_c}}\\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}) = \\frac{\\sum_{x=1}^{V}exp(\\mathbf{u_{x}^{T} v_{c}}) \\cdot u_{x}}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})}\\tag{Step 4}$$\n",
    "\n",
    "Step 4 can be rearranged as:\n",
    "\n",
    "$$\\sum_{x=1}^{V} \\frac{exp(\\mathbf{u_{x}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\cdot  u_{x} \\tag{Step 5}$$\n",
    "\n",
    "Recall the prediction function (3):\n",
    "\n",
    "$$P(o|c) = \\frac{exp(\\mathbf{u_{o}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\tag{3}$$\n",
    "\n",
    "We can rearrange Step 5 result accordingly:\n",
    "\n",
    "$$\\sum_{x=1}^{V} P(x|c) \\cdot  u_{x} \\tag{Step 6}$$\n",
    "\n",
    "Taking everything together (Step 6 + Step 1), finally we get:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{v_c}} (- \\frac{1}{T}(\\mathbf{u_{o}^{T} v_{c}}- \\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}))) = - \\frac{1}{T}(\\mathbf{u_o} - \\sum_{x=1}^{V} P(x|c) \\cdot  \\mathbf{u_{x}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11840529",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590185e1",
   "metadata": {},
   "source": [
    "### Partial derivative with regard to $u_o$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u_o}} (- \\frac{1}{T}(\\mathbf{u_{o}^{T} v_{c}}- \\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}))) = - \\frac{1}{T} (\\frac{\\partial}{\\partial \\mathbf{u_o}}\\mathbf{u_{o}^{T} v_{c}} -  \\frac{\\partial}{\\partial \\mathbf{u_o}}\\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u_o}}\\mathbf{u_{o}^{T} v_{c}} = \\mathbf{v_{c}} \\tag{Step 1}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf{u_o}}\\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}) = \\frac{1}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\cdot \\sum_{x=1}^{V} \\frac{\\partial}{\\partial \\mathbf{u_x}}exp(\\mathbf{u_{x}^{T} v_{c}})\\tag{Step 2}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u_x}}exp(\\mathbf{u_{x}^{T} v_{c}}) = exp(\\mathbf{u_{x}^{T} v_{c}}) \\cdot  \\mathbf{v_{c}}\\tag{Step 3}$$\n",
    "\n",
    "Taking Step 2 and Step3 together, we get:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u_o}}\\mathbf{u_{o}^{T} v_{c}} = \\frac{\\sum_{x=1}^{V}exp(\\mathbf{u_{x}^{T} v_{c}}) \\cdot  \\mathbf{v_{c}}}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} \\tag{Step 4}$$\n",
    "\n",
    "Step 4 can be rearranged as:\n",
    "\n",
    "$$\\frac{\\sum_{x=1}^{V}exp(\\mathbf{u_{x}^{T} v_{c}}) \\cdot  \\mathbf{v_{c}}}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})} = \\sum_{x=1}^{V}\\frac{exp(\\mathbf{u_{x}^{T} v_{c}})}{\\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}})}\\mathbf{v_{c}} = \\sum_{x=1}^{V} P(x|c) \\cdot \\mathbf{v_{c}}\\tag{Step 5}$$\n",
    "\n",
    "\n",
    "Finally, everything taken together:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{u_o}} (- \\frac{1}{T}(\\mathbf{u_{o}^{T} v_{c}}- \\log \\sum_{w \\in V} exp(\\mathbf{u_{w}^{T} v_{c}}))) = - \\frac{1}{T}(\\mathbf{u_o} - \\sum_{x=1}^{V} P(x|c) \\cdot  \\mathbf{v_{c}})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
